{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a108fc3e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BERT Base Uncased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch : 0\n",
      "End epoch : 0\n",
      "Start epoch : 1\n",
      "End epoch : 1\n",
      "Start epoch : 2\n",
      "End epoch : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/monu/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RoBERTa Base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch : 0\n",
      "End epoch : 0\n",
      "Start epoch : 1\n",
      "End epoch : 1\n",
      "Start epoch : 2\n",
      "End epoch : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/monu/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating DistilBERT Base Uncased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch : 0\n",
      "End epoch : 0\n",
      "Start epoch : 1\n",
      "End epoch : 1\n",
      "Start epoch : 2\n",
      "End epoch : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/monu/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BERT Large Cased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch : 0\n",
      "End epoch : 0\n",
      "Start epoch : 1\n",
      "End epoch : 1\n",
      "Start epoch : 2\n",
      "End epoch : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/monu/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ALBERT Base V2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch : 0\n",
      "End epoch : 0\n",
      "Start epoch : 1\n",
      "End epoch : 1\n",
      "Start epoch : 2\n",
      "End epoch : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/monu/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating DistilBERT Base Cased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch : 0\n",
      "End epoch : 0\n",
      "Start epoch : 1\n",
      "End epoch : 1\n",
      "Start epoch : 2\n",
      "End epoch : 2\n",
      "Evaluation Metrics:\n",
      "                         Accuracy  Precision    Recall  F1 Score\n",
      "BERT Base Uncased        0.204250   0.168618  0.204250  0.102458\n",
      "RoBERTa Base             0.206021   0.042445  0.206021  0.070388\n",
      "DistilBERT Base Uncased  0.197757   0.162036  0.197757  0.144189\n",
      "BERT Large Cased         0.206021   0.042445  0.206021  0.070388\n",
      "ALBERT Base V2           0.206021   0.042445  0.206021  0.070388\n",
      "DistilBERT Base Cased    0.202479   0.201535  0.202479  0.109438\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (BertTokenizer, BertForSequenceClassification,\n",
    "                          RobertaTokenizer, RobertaForSequenceClassification,\n",
    "                          DistilBertTokenizer, DistilBertForSequenceClassification,\n",
    "                          AlbertTokenizer, AlbertForSequenceClassification)\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)  # if you use multi-GPU\n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Load the provided ticket dataset\n",
    "df = pd.read_csv('customer_support_tickets.csv')\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define BERT-based models\n",
    "bert_models = [\n",
    "    ('BERT Base Uncased', 'bert-base-uncased'),\n",
    "    ('RoBERTa Base', 'roberta-base'),\n",
    "    ('DistilBERT Base Uncased', 'distilbert-base-uncased'),\n",
    "    ('BERT Large Cased', 'bert-large-cased'),\n",
    "    ('ALBERT Base V2', 'albert-base-v2'),\n",
    "    ('DistilBERT Base Cased', 'distilbert-base-cased')\n",
    "]\n",
    "\n",
    "# Define a dictionary to store evaluation metrics for each model\n",
    "evaluation_results = {}\n",
    "\n",
    "# Iterate over BERT-based models\n",
    "for model_name, model_type in bert_models:\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    if 'roberta' in model_type:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_type)\n",
    "        model = RobertaForSequenceClassification.from_pretrained(model_type, num_labels=len(df['Ticket Type'].unique()))\n",
    "    elif 'distilbert' in model_type:\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(model_type)\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(model_type, num_labels=len(df['Ticket Type'].unique()))\n",
    "    elif 'albert' in model_type:\n",
    "        tokenizer = AlbertTokenizer.from_pretrained(model_type)\n",
    "        model = AlbertForSequenceClassification.from_pretrained(model_type, num_labels=len(df['Ticket Type'].unique()))\n",
    "    else:\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_type)\n",
    "        model = BertForSequenceClassification.from_pretrained(model_type, num_labels=len(df['Ticket Type'].unique()))\n",
    "    \n",
    "    # Tokenize ticket descriptions and convert labels to indices\n",
    "    train_encodings = tokenizer(train_df['Ticket Description'].tolist(), truncation=True, padding=True)\n",
    "    test_encodings = tokenizer(test_df['Ticket Description'].tolist(), truncation=True, padding=True)\n",
    "    train_labels = torch.tensor(train_df['Ticket Type'].astype('category').cat.codes.tolist())\n",
    "    test_labels = torch.tensor(test_df['Ticket Type'].astype('category').cat.codes.tolist())\n",
    "    \n",
    "    # Create PyTorch datasets and data loaders\n",
    "    train_dataset = TensorDataset(torch.tensor(train_encodings['input_ids']), torch.tensor(train_encodings['attention_mask']), train_labels)\n",
    "    test_dataset = TensorDataset(torch.tensor(test_encodings['input_ids']), torch.tensor(test_encodings['attention_mask']), test_labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "    \n",
    "    # Fine-tune the model\n",
    "    #optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "    model.train()\n",
    "    for epoch in range(3):  # example of training for 3 epochs\n",
    "        print(\"Start epoch : {}\".format(epoch))\n",
    "        for batch in train_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"End epoch : {}\".format(epoch))\n",
    "    # Evaluate the model on the test data\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predicted_labels = torch.argmax(logits, dim=1)\n",
    "            predictions.extend(predicted_labels.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='weighted')\n",
    "    recall = recall_score(true_labels, predictions, average='weighted')\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    \n",
    "    # Store evaluation results\n",
    "    evaluation_results[model_name] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1 Score': f1}\n",
    "\n",
    "# Convert evaluation results to a DataFrame\n",
    "evaluation_df = pd.DataFrame.from_dict(evaluation_results, orient='index')\n",
    "\n",
    "# Print evaluation metrics in tabular form\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(evaluation_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7ed080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
