{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2aa81b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BERT Base Uncased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RoBERTa Base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/monu/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating DistilBERT Base Uncased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Twitter RoBERTa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "                         Accuracy  Precision    Recall  F1 Score\n",
      "BERT Base Uncased        0.393211   0.237368  0.393211  0.246992\n",
      "RoBERTa Base             0.302687   0.091620  0.302687  0.140663\n",
      "DistilBERT Base Uncased  0.295615   0.306951  0.295615  0.161411\n",
      "Twitter RoBERTa          0.711457   0.715022  0.711457  0.704682\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import os\n",
    "# Set random seeds for reproducibility\n",
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value) \n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "# Load the sentiment dataset\n",
    "df = pd.read_csv('Sentimental.csv',encoding='ISO-8859-1')\n",
    "\n",
    "# Drop rows with missing text data\n",
    "df.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models and their corresponding tokenizers\n",
    "models = [\n",
    "    ('BERT Base Uncased', 'bert-base-uncased'),\n",
    "    ('RoBERTa Base', 'roberta-base'),\n",
    "    ('DistilBERT Base Uncased', 'distilbert-base-uncased'),\n",
    "    ('Twitter RoBERTa', 'cardiffnlp/twitter-roberta-base-sentiment'),\n",
    "\n",
    "]\n",
    "\n",
    "# Define a dictionary to store evaluation metrics and predictions for each model\n",
    "evaluation_results = {}\n",
    "\n",
    "# Iterate over models\n",
    "for model_name, model_type in models:\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    if 'roberta' in model_type:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_type)\n",
    "        model = RobertaForSequenceClassification.from_pretrained(model_type, num_labels=len(df['sentiment'].unique()))\n",
    "    elif 'distilbert' in model_type:\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(model_type)\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(model_type, num_labels=len(df['sentiment'].unique()))\n",
    "    elif 'twitter-roberta' in model_type:  # Add Twitter-specific model condition\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_type)\n",
    "    else:\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_type)\n",
    "        model = BertForSequenceClassification.from_pretrained(model_type, num_labels=len(df['sentiment'].unique()))\n",
    "    \n",
    "    # Tokenize text and convert labels to indices\n",
    "    test_encodings = tokenizer(test_df['text'].tolist(), truncation=True, padding=True)\n",
    "    test_labels = torch.tensor(test_df['sentiment'].astype('category').cat.codes.tolist())\n",
    "    \n",
    "    # Create PyTorch dataset and data loader\n",
    "    test_dataset = TensorDataset(torch.tensor(test_encodings['input_ids']), torch.tensor(test_encodings['attention_mask']), test_labels)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "    \n",
    "    # Evaluate the model on the test data\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predicted_labels = torch.argmax(logits, dim=1)\n",
    "            predictions.extend(predicted_labels.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='weighted')\n",
    "    recall = recall_score(true_labels, predictions, average='weighted')\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    \n",
    "    # Store evaluation results including predictions and true labels\n",
    "    evaluation_results[model_name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Predictions': predictions,\n",
    "        'True Labels': true_labels\n",
    "    }\n",
    "    evaluation_results[model_name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Predictions': predictions,\n",
    "\n",
    "    }\n",
    "    \n",
    "    evaluation_results[model_name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "\n",
    "    }\n",
    "# Print evaluation metrics in tabular form\n",
    "print(\"Evaluation Metrics:\")\n",
    "evaluation_df = pd.DataFrame.from_dict(evaluation_results, orient='index')\n",
    "print(evaluation_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a74026",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
