{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOv8CAAy/bSruawHxNUVP3h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shajivmasters/Thesis/blob/main/1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"customer.csv\")\n",
        "# Take a sample of 500 rows\n",
        "data = data.sample(n=200, random_state=42)\n",
        "# Preprocess the data\n",
        "data = data.dropna(subset=['Ticket Description'])  # Drop rows with missing values in the target column\n",
        "data = data[['Ticket Description', 'Ticket Type']]  # Select relevant columns\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(data['Ticket Description'], data['Ticket Type'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
        "test_labels_encoded = label_encoder.transform(test_labels)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
        "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True)\n",
        "\n",
        "# Convert encodings to a format suitable for the trainer\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = MyDataset(train_encodings, train_labels_encoded)\n",
        "eval_dataset = MyDataset(test_encodings, test_labels_encoded)\n",
        "\n",
        "# Define BERT model (Pre-trained)\n",
        "model_pretrained = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
        "\n",
        "# Define training arguments for pre-trained model\n",
        "training_args_pretrained = TrainingArguments(\n",
        "    output_dir='./results_pretrained',\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    evaluation_strategy='epoch',\n",
        "    logging_dir='./logs_pretrained',\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    save_strategy='epoch',  # Set save strategy to 'epoch'\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "# Define trainer for pre-trained model\n",
        "trainer_pretrained = Trainer(\n",
        "    model=model_pretrained,\n",
        "    args=training_args_pretrained,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset\n",
        ")\n",
        "\n",
        "# Train and evaluate pre-trained model\n",
        "start_time = time.time()\n",
        "trainer_pretrained.train()\n",
        "preds_pretrained = trainer_pretrained.predict(eval_dataset)\n",
        "predictions_pretrained = label_encoder.inverse_transform(preds_pretrained.predictions.argmax(axis=1))\n",
        "accuracy_pretrained = accuracy_score(test_labels, predictions_pretrained)\n",
        "precision_pretrained = precision_score(test_labels, predictions_pretrained, average='weighted')\n",
        "recall_pretrained = recall_score(test_labels, predictions_pretrained, average='weighted')\n",
        "f1_pretrained = f1_score(test_labels, predictions_pretrained, average='weighted')\n",
        "pretrained_processing_time = time.time() - start_time\n",
        "\n",
        "# Define BERT model (Fine-tuned by skipping intermediate layers)\n",
        "model_finetuned = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
        "\n",
        "# Fine-tune by skipping intermediate layers\n",
        "model_finetuned.bert.encoder.layer = model_finetuned.bert.encoder.layer[:-2]\n",
        "\n",
        "# Define training arguments for fine-tuned model\n",
        "training_args_finetuned = TrainingArguments(\n",
        "    output_dir='./results_finetuned',\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    evaluation_strategy='epoch',\n",
        "    logging_dir='./logs_finetuned',\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    save_strategy='epoch',  # Set save strategy to 'epoch'\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "# Define trainer for fine-tuned model\n",
        "trainer_finetuned = Trainer(\n",
        "    model=model_finetuned,\n",
        "    args=training_args_finetuned,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset\n",
        ")\n",
        "\n",
        "# Train and evaluate fine-tuned model\n",
        "start_time = time.time()\n",
        "trainer_finetuned.train()\n",
        "preds_finetuned = trainer_finetuned.predict(eval_dataset)\n",
        "predictions_finetuned = label_encoder.inverse_transform(preds_finetuned.predictions.argmax(axis=1))\n",
        "accuracy_finetuned = accuracy_score(test_labels, predictions_finetuned)\n",
        "precision_finetuned = precision_score(test_labels, predictions_finetuned, average='weighted')\n",
        "recall_finetuned = recall_score(test_labels, predictions_finetuned, average='weighted')\n",
        "f1_finetuned = f1_score(test_labels, predictions_finetuned, average='weighted')\n",
        "finetuned_processing_time = time.time() - start_time\n",
        "\n",
        "# Store results in a DataFrame\n",
        "results = pd.DataFrame({\n",
        "    \"Model\": [\"Pre-trained BERT\", \"Fine-tuned BERT (skipping intermediate layers)\"],\n",
        "    \"Accuracy\": [accuracy_pretrained, accuracy_finetuned],\n",
        "    \"Precision\": [precision_pretrained, precision_finetuned],\n",
        "    \"Recall\": [recall_pretrained, recall_finetuned],\n",
        "    \"F1-score\": [f1_pretrained, f1_finetuned],\n",
        "    \"Processing Time (seconds)\": [pretrained_processing_time, finetuned_processing_time]\n",
        "})\n",
        "\n",
        "# Print results\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "id": "ckXn78YPxHd7",
        "outputId": "a9ddedb8-2f67-4e5c-82af-2b4c36dd152d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 09:27, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.624141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.591408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.592376</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 08:12, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.605580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.641241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.613981</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            Model  Accuracy  Precision  \\\n",
            "0                                Pre-trained BERT     0.325   0.251915   \n",
            "1  Fine-tuned BERT (skipping intermediate layers)     0.250   0.195211   \n",
            "\n",
            "   Recall  F1-score  Processing Time (seconds)  \n",
            "0   0.325  0.250602                 588.745347  \n",
            "1   0.250  0.198276                 510.935875  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"customer.csv\")\n",
        "# Take a sample of 200 rows\n",
        "data = data.sample(n=200, random_state=42)\n",
        "# Preprocess the data\n",
        "data = data.dropna(subset=['Ticket Description'])  # Drop rows with missing values in the target column\n",
        "data = data[['Ticket Description', 'Ticket Type']]  # Select relevant columns\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(data['Ticket Description'], data['Ticket Type'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
        "test_labels_encoded = label_encoder.transform(test_labels)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
        "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True)\n",
        "\n",
        "# Convert encodings to a format suitable for the trainer\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = MyDataset(train_encodings, train_labels_encoded)\n",
        "eval_dataset = MyDataset(test_encodings, test_labels_encoded)\n",
        "\n",
        "# Define BERT model (Pre-trained)\n",
        "model_pretrained = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
        "\n",
        "# Define training arguments for pre-trained model\n",
        "training_args_pretrained = TrainingArguments(\n",
        "    output_dir='./results_pretrained',\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    evaluation_strategy='epoch',\n",
        "    logging_dir='./logs_pretrained',\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    save_strategy='epoch',  # Set save strategy to 'epoch'\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "# Define trainer for pre-trained model\n",
        "trainer_pretrained = Trainer(\n",
        "    model=model_pretrained,\n",
        "    args=training_args_pretrained,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset\n",
        ")\n",
        "\n",
        "# Train and evaluate pre-trained model\n",
        "start_time = time.time()\n",
        "trainer_pretrained.train()\n",
        "preds_pretrained = trainer_pretrained.predict(eval_dataset)\n",
        "predictions_pretrained = label_encoder.inverse_transform(preds_pretrained.predictions.argmax(axis=1))\n",
        "accuracy_pretrained = accuracy_score(test_labels, predictions_pretrained)\n",
        "precision_pretrained = precision_score(test_labels, predictions_pretrained, average='weighted')\n",
        "recall_pretrained = recall_score(test_labels, predictions_pretrained, average='weighted')\n",
        "f1_pretrained = f1_score(test_labels, predictions_pretrained, average='weighted')\n",
        "pretrained_processing_time = time.time() - start_time\n",
        "\n",
        "# Define BERT model (Fine-tuned by skipping intermediate layers)\n",
        "model_finetuned = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
        "\n",
        "# Fine-tune by skipping intermediate layers\n",
        "layers_to_include = [0, 2, 4]  # Layers to include (0-indexed)\n",
        "encoder_layers = model_finetuned.bert.encoder.layer\n",
        "layers_to_remove = [i for i in range(len(encoder_layers)) if i not in layers_to_include]\n",
        "model_finetuned.bert.encoder.layer = torch.nn.ModuleList([layer for i, layer in enumerate(encoder_layers) if i in layers_to_include])\n",
        "\n",
        "# Define training arguments for fine-tuned model\n",
        "training_args_finetuned = TrainingArguments(\n",
        "    output_dir='./results_finetuned',\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    evaluation_strategy='epoch',\n",
        "    logging_dir='./logs_finetuned',\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    save_strategy='epoch',  # Set save strategy to 'epoch'\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "# Define trainer for fine-tuned model\n",
        "trainer_finetuned = Trainer(\n",
        "    model=model_finetuned,\n",
        "    args=training_args_finetuned,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset\n",
        ")\n",
        "\n",
        "# Train and evaluate fine-tuned model\n",
        "start_time = time.time()\n",
        "trainer_finetuned.train()\n",
        "preds_finetuned = trainer_finetuned.predict(eval_dataset)\n",
        "predictions_finetuned = label_encoder.inverse_transform(preds_finetuned.predictions.argmax(axis=1))\n",
        "accuracy_finetuned = accuracy_score(test_labels, predictions_finetuned)\n",
        "precision_finetuned = precision_score(test_labels, predictions_finetuned, average='weighted')\n",
        "recall_finetuned = recall_score(test_labels, predictions_finetuned, average='weighted')\n",
        "f1_finetuned = f1_score(test_labels, predictions_finetuned, average='weighted')\n",
        "finetuned_processing_time = time.time() - start_time\n",
        "\n",
        "# Store results in a DataFrame\n",
        "results = pd.DataFrame({\n",
        "    \"Model\": [\"Pre-trained BERT\", \"Fine-tuned BERT (skipping intermediate layers)\"],\n",
        "    \"Accuracy\": [accuracy_pretrained, accuracy_finetuned],\n",
        "    \"Precision\": [precision_pretrained, precision_finetuned],\n",
        "    \"Recall\": [recall_pretrained, recall_finetuned],\n",
        "    \"F1-score\": [f1_pretrained, f1_finetuned],\n",
        "    \"Processing Time (seconds)\": [pretrained_processing_time, finetuned_processing_time]\n",
        "})\n",
        "\n",
        "# Print results\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "id": "jxtIqneKBlR4",
        "outputId": "0cbf5b97-5a1c-40bb-daf0-07c3f49ffb76"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 09:48, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.628249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.630643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.629398</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checkpoint destination directory ./results_pretrained/checkpoint-20 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory ./results_pretrained/checkpoint-40 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory ./results_pretrained/checkpoint-60 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 02:49, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.617752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.612645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.601656</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checkpoint destination directory ./results_finetuned/checkpoint-20 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory ./results_finetuned/checkpoint-40 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory ./results_finetuned/checkpoint-60 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            Model  Accuracy  Precision  \\\n",
            "0                                Pre-trained BERT     0.225   0.106731   \n",
            "1  Fine-tuned BERT (skipping intermediate layers)     0.300   0.090000   \n",
            "\n",
            "   Recall  F1-score  Processing Time (seconds)  \n",
            "0   0.225  0.144737                 616.980701  \n",
            "1   0.300  0.138462                 175.606799  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Now intermediate layer skipping\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"customer.csv\")\n",
        "\n",
        "# Group the data by 'Ticket Type' and sample an equal number of rows from each category\n",
        "grouped_data = data.groupby('Ticket Type', group_keys=False).apply(lambda x: x.sample(min(len(x), 40), random_state=42))\n",
        "\n",
        "# Preprocess the data\n",
        "grouped_data = grouped_data.dropna(subset=['Ticket Description'])  # Drop rows with missing values in the target column\n",
        "grouped_data = grouped_data[['Ticket Description', 'Ticket Type']]  # Select relevant columns\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(grouped_data['Ticket Description'], grouped_data['Ticket Type'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
        "test_labels_encoded = label_encoder.transform(test_labels)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
        "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True)\n",
        "\n",
        "# Convert encodings to a format suitable for the trainer\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = MyDataset(train_encodings, train_labels_encoded)\n",
        "eval_dataset = MyDataset(test_encodings, test_labels_encoded)\n",
        "\n",
        "\n",
        "# Define BERT model (Pre-trained)\n",
        "model_pretrained = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
        "\n",
        "# Define training arguments for pre-trained model\n",
        "training_args_pretrained = TrainingArguments(\n",
        "    output_dir='./results_pretrained',\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    evaluation_strategy='epoch',\n",
        "    logging_dir='./logs_pretrained',\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    save_strategy='epoch',  # Set save strategy to 'epoch'\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "# Define trainer for pre-trained model\n",
        "trainer_pretrained = Trainer(\n",
        "    model=model_pretrained,\n",
        "    args=training_args_pretrained,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset\n",
        ")\n",
        "\n",
        "# Train and evaluate pre-trained model\n",
        "start_time = time.time()\n",
        "trainer_pretrained.train()\n",
        "preds_pretrained = trainer_pretrained.predict(eval_dataset)\n",
        "predictions_pretrained = label_encoder.inverse_transform(preds_pretrained.predictions.argmax(axis=1))\n",
        "accuracy_pretrained = accuracy_score(test_labels, predictions_pretrained)\n",
        "precision_pretrained = precision_score(test_labels, predictions_pretrained, average='weighted')\n",
        "recall_pretrained = recall_score(test_labels, predictions_pretrained, average='weighted')\n",
        "f1_pretrained = f1_score(test_labels, predictions_pretrained, average='weighted')\n",
        "pretrained_processing_time = time.time() - start_time\n",
        "\n",
        "\n",
        "# Define BERT model (Fine-tuned by skipping intermediate layers)\n",
        "model_finetuned = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
        "\n",
        "# Fine-tune by skipping intermediate layers\n",
        "layers_to_include = [0, 2, 4, 6, 8, 10]  # Include only even-numbered layers (0-indexed)\n",
        "encoder_layers = model_finetuned.bert.encoder.layer\n",
        "model_finetuned.bert.encoder.layer = torch.nn.ModuleList([layer for i, layer in enumerate(encoder_layers) if i in layers_to_include])\n",
        "\n",
        "# Define training arguments for fine-tuned model\n",
        "training_args_finetuned = TrainingArguments(\n",
        "    output_dir='./results_finetuned',\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    evaluation_strategy='epoch',\n",
        "    logging_dir='./logs_finetuned',\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    save_strategy='epoch',  # Set save strategy to 'epoch'\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "# Define trainer for fine-tuned model\n",
        "trainer_finetuned = Trainer(\n",
        "    model=model_finetuned,\n",
        "    args=training_args_finetuned,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset\n",
        ")\n",
        "\n",
        "# Train and evaluate fine-tuned model\n",
        "start_time = time.time()\n",
        "trainer_finetuned.train()\n",
        "preds_finetuned = trainer_finetuned.predict(eval_dataset)\n",
        "predictions_finetuned = label_encoder.inverse_transform(preds_finetuned.predictions.argmax(axis=1))\n",
        "accuracy_finetuned = accuracy_score(test_labels, predictions_finetuned)\n",
        "precision_finetuned = precision_score(test_labels, predictions_finetuned, average='weighted')\n",
        "recall_finetuned = recall_score(test_labels, predictions_finetuned, average='weighted')\n",
        "f1_finetuned = f1_score(test_labels, predictions_finetuned, average='weighted')\n",
        "finetuned_processing_time = time.time() - start_time\n",
        "\n",
        "# Store results in a DataFrame\n",
        "# Store results in a DataFrame\n",
        "results = pd.DataFrame({\n",
        "    \"Model\": [\"Pre-trained BERT\", \"Fine-tuned BERT (skipping intermediate layers)\"],\n",
        "    \"Accuracy\": [accuracy_pretrained, accuracy_finetuned],\n",
        "    \"Precision\": [precision_pretrained, precision_finetuned],\n",
        "    \"Recall\": [recall_pretrained, recall_finetuned],\n",
        "    \"F1-score\": [f1_pretrained, f1_finetuned],\n",
        "    \"Processing Time (seconds)\": [pretrained_processing_time, finetuned_processing_time]\n",
        "})\n",
        "# Print results\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "id": "djlhXmhrK0up",
        "outputId": "02bafc24-2552-4872-b637-6415bd2bbafc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 09:29, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.675156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.671737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.666355</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checkpoint destination directory ./results_pretrained/checkpoint-20 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory ./results_pretrained/checkpoint-40 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory ./results_pretrained/checkpoint-60 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 05:14, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.656739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.656206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.660780</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checkpoint destination directory ./results_finetuned/checkpoint-20 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory ./results_finetuned/checkpoint-40 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory ./results_finetuned/checkpoint-60 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            Model  Accuracy  Precision  \\\n",
            "0                                Pre-trained BERT      0.10   0.036364   \n",
            "1  Fine-tuned BERT (skipping intermediate layers)      0.15   0.058986   \n",
            "\n",
            "   Recall  F1-score  Processing Time (seconds)  \n",
            "0    0.10  0.051852                 602.671089  \n",
            "1    0.15  0.073932                 329.443756  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Run for whole data\n",
        "\n",
        "## Now intermediate layer skipping\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"customer.csv\")\n",
        "\n",
        "# Preprocess the data\n",
        "data = data.dropna(subset=['Ticket Description'])  # Drop rows with missing values in the target column\n",
        "data = data[['Ticket Description', 'Ticket Type']]  # Select relevant columns\n",
        "\n",
        "\n",
        "# Group the data by 'Ticket Type' and sample an equal number of rows from each category\n",
        "#grouped_data = data.groupby('Ticket Type', group_keys=False).apply(lambda x: x.sample(min(len(x), 40), random_state=42))\n",
        "\n",
        "# Preprocess the data\n",
        "#grouped_data = grouped_data.dropna(subset=['Ticket Description'])  # Drop rows with missing values in the target column\n",
        "#grouped_data = grouped_data[['Ticket Description', 'Ticket Type']]  # Select relevant columns\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(data['Ticket Description'], data['Ticket Type'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
        "test_labels_encoded = label_encoder.transform(test_labels)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
        "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True)\n",
        "\n",
        "# Convert encodings to a format suitable for the trainer\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = MyDataset(train_encodings, train_labels_encoded)\n",
        "eval_dataset = MyDataset(test_encodings, test_labels_encoded)\n",
        "\n",
        "\n",
        "# Define BERT model (Pre-trained)\n",
        "model_pretrained = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
        "\n",
        "# Define training arguments for pre-trained model\n",
        "training_args_pretrained = TrainingArguments(\n",
        "    output_dir='./results_pretrained',\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    evaluation_strategy='epoch',\n",
        "    logging_dir='./logs_pretrained',\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    save_strategy='epoch',  # Set save strategy to 'epoch'\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "# Define trainer for pre-trained model\n",
        "trainer_pretrained = Trainer(\n",
        "    model=model_pretrained,\n",
        "    args=training_args_pretrained,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset\n",
        ")\n",
        "\n",
        "# Train and evaluate pre-trained model\n",
        "start_time = time.time()\n",
        "trainer_pretrained.train()\n",
        "preds_pretrained = trainer_pretrained.predict(eval_dataset)\n",
        "predictions_pretrained = label_encoder.inverse_transform(preds_pretrained.predictions.argmax(axis=1))\n",
        "accuracy_pretrained = accuracy_score(test_labels, predictions_pretrained)\n",
        "precision_pretrained = precision_score(test_labels, predictions_pretrained, average='weighted')\n",
        "recall_pretrained = recall_score(test_labels, predictions_pretrained, average='weighted')\n",
        "f1_pretrained = f1_score(test_labels, predictions_pretrained, average='weighted')\n",
        "pretrained_processing_time = time.time() - start_time\n",
        "\n",
        "\n",
        "# Define BERT model (Fine-tuned by skipping intermediate layers)\n",
        "model_finetuned = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
        "\n",
        "# Fine-tune by skipping intermediate layers\n",
        "layers_to_include = [0, 2, 4, 6, 8, 10]  # Include only even-numbered layers (0-indexed)\n",
        "encoder_layers = model_finetuned.bert.encoder.layer\n",
        "model_finetuned.bert.encoder.layer = torch.nn.ModuleList([layer for i, layer in enumerate(encoder_layers) if i in layers_to_include])\n",
        "\n",
        "# Define training arguments for fine-tuned model\n",
        "training_args_finetuned = TrainingArguments(\n",
        "    output_dir='./results_finetuned',\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    evaluation_strategy='epoch',\n",
        "    logging_dir='./logs_finetuned',\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    save_strategy='epoch',  # Set save strategy to 'epoch'\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "# Define trainer for fine-tuned model\n",
        "trainer_finetuned = Trainer(\n",
        "    model=model_finetuned,\n",
        "    args=training_args_finetuned,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset\n",
        ")\n",
        "\n",
        "# Train and evaluate fine-tuned model\n",
        "start_time = time.time()\n",
        "trainer_finetuned.train()\n",
        "preds_finetuned = trainer_finetuned.predict(eval_dataset)\n",
        "predictions_finetuned = label_encoder.inverse_transform(preds_finetuned.predictions.argmax(axis=1))\n",
        "accuracy_finetuned = accuracy_score(test_labels, predictions_finetuned)\n",
        "precision_finetuned = precision_score(test_labels, predictions_finetuned, average='weighted')\n",
        "recall_finetuned = recall_score(test_labels, predictions_finetuned, average='weighted')\n",
        "f1_finetuned = f1_score(test_labels, predictions_finetuned, average='weighted')\n",
        "finetuned_processing_time = time.time() - start_time\n",
        "\n",
        "# Store results in a DataFrame\n",
        "# Store results in a DataFrame\n",
        "results = pd.DataFrame({\n",
        "    \"Model\": [\"Pre-trained BERT\", \"Fine-tuned BERT (skipping intermediate layers)\"],\n",
        "    \"Accuracy\": [accuracy_pretrained, accuracy_finetuned],\n",
        "    \"Precision\": [precision_pretrained, precision_finetuned],\n",
        "    \"Recall\": [recall_pretrained, recall_finetuned],\n",
        "    \"F1-score\": [f1_pretrained, f1_finetuned],\n",
        "    \"Processing Time (seconds)\": [pretrained_processing_time, finetuned_processing_time]\n",
        "})\n",
        "# Print results\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "ASgqOdAtOTyP",
        "outputId": "47463c16-9a6e-4907-e5cf-19f3c53bde92"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='802' max='2541' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 802/2541 3:52:05 < 8:24:29, 0.06 it/s, Epoch 0.95/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='814' max='2541' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 814/2541 3:55:35 < 8:21:04, 0.06 it/s, Epoch 0.96/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZOvKJvOmIJYz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TwTA7zzQO7sh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}